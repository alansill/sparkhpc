#!/bin/sh
#BSUB -J spark-ngs
#BSUB -W {walltime} # runtime to request
#BSUB -o spark-ngs-%J.log # output extra o means overwrite
#BSUB -n {ncores} # requesting ncores cores
#BSUB -R "span[hosts=-1] rusage[mem={mem}]" # take any available core with memmb memory in MB

#module load new
#module load java
#module load open_mpi

#source setup_environment.sh

# setup the spark paths
export SPARK_HOME=$HOME/spark

# custom python installation path
#export PATH=$HOME/miniconda/bin:$PATH

# initialize the nodes -- run ./start_spark_lsf.py -h to see other options
export SPARK_LOCAL_DIRS="$TMPDIR /scratch/spark"
export LOCAL_DIRS="$TMPDIR, /scratch/spark"
export SPARK_WORKER_DIR="$TMPDIR/work, /scratch/spark/work"

start_spark_lsf.py 24 {mem}

