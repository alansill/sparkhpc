#!/bin/env python
#
#
# CLI for starting and running Spark standalone clusters on HPC resources
#
#

from __future__ import print_function
import click
from sparkhpc import sparkjob
import subprocess

@click.group()
@click.option('--scheduler', type=click.Choice(['lsf']), default='lsf', help='Which scheduler to use')
@click.pass_context
def cli(ctx, scheduler):
    ctx.obj['SJ'] = sparkjob.sparkjob_factory(scheduler)


@cli.command()
@click.argument('ncores')
@click.option('--walltime', default="00:30", help="Walltime in HH:MM format")
@click.option('--jobname', default='spark', help='Name to use for the job')
@click.option('--template', default=None, help='Job template path')
@click.option('--memory', default='2000', help='Memory for each worker in MB', envvar='SPARK_WORKER_MEMORY')
@click.option('--wait/--no-wait', default=False, help='Wait until the job starts')
@click.pass_context
def submit(ctx, ncores, walltime, jobname, template, memory, wait):
    """Submit the spark cluster as an LSF job"""
    
    SJ = ctx.obj['SJ']

    sj = SJ(ncores=ncores, walltime=walltime, jobname=jobname, template=template, memory=memory)
    
    if wait: 
        print('Waiting for job to start - ctrl-c to stop')
        sj.wait_to_start()
    else:
        sj.submit()
    

@cli.command()
@click.pass_context
def info(ctx):
    """Get info about currently running clusters"""
    SJ = ctx.obj['SJ']
    sjs = SJ.current_clusters()

    if len(sjs) == 0: 
        print('No Spark clusters found')

    for i,sj in enumerate(sjs): 
        print('----- Cluster %d -----'%i)
        if sj.job_started(): 
            print('Number of cores: %s'%sj.ncores)
            print('master URL: %s'%sj.master_url()[0])
            print('Spark UI: %s'%sj.master_ui()[0])
        else: 
            print('Job %s not yet started'%sj.jobid)


@cli.command()
@click.argument('clusterid')
@click.pass_context
def stop(ctx, clusterid):
    """Kill a currently running cluster ('all' to kill all clusters)"""
    SJ = ctx.obj['SJ']
    sjs = SJ.current_clusters()

    if clusterid == 'all': 
        for sj in sjs: 
            sj.stop()
    elif int(clusterid) < len(sjs): 
        sjs[int(clusterid)].stop()
    else: 
        raise RuntimeError('Cluster %s does not exist'%clusterid)


@cli.command()
@click.option('--memory', default='2000', help='Memory for each worker in MB')
@click.pass_context
def start(ctx, mem):
    """Start the Spark cluster within a current job context"""
    sparkjob.start_cluster(mem)

if __name__ == "__main__":
    cli(obj={})
